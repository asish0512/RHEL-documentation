= Deploy a Kubernetes cluster on RHEL 7.5 for s390x architecture.

For this documentation two VMS has been used for deployment and testing pupose. One can extend it to any number of worker nodes.
Out of these two VMs one acts as master(controller) and other acts as slave(worker). Create two instances on LinuxONE community cloud and ssh using the respective keys.


A kubernetes cluster consists of these essential components

....
1. etcd (distributed key-value storage)
2. apiserver
3. scheduler
4. controller manager
5. kubectl (command line client for kubernetes)
5. Docker (container)
6. kubelet
7. kube-proxy
....

The following components have been used.
[%header,cols=2*,width="30%"]
|===

|Instances
|RHEL 7.5 flavour of L1CC
|ETCD
|3.2.22
|Docker CE
|17.05
|Kubernetes
|1.9.8
|Flannel
|0.7.1
|===

Do this to boot using the latest kernel

....
mv /etc/yum.repos.d/rhel7.repo /root
....

To be able to download packages you need to register your instance and for that you need a RHEL account. Also enable the repos from which you can download
the kubernetes components.

....
subscription-manager register
subscription-manager attach
subscription-manager repos --enable=rhel-7-for-system-z-extras-rpms
subscription-manager repos --list-enabled
....

NOTE: All these commands have been run as root user.

== Prerequisites
*Basic tools like git, curl etc*

 During the process of deployment you may need some basic linux tools like curl, git, wget, tar, gcc and which. In RHEL they have all been preinstalled.



== Binaries

*Set the following variables on both the nodes*
....
TAG=v1.9.8
URL=https://storage.googleapis.com/kubernetes-release/release/$TAG/bin/linux/s390x
....

*Binaries to be installed on Master Node*

....
curl -# -L -o /usr/bin/kube-apiserver $URL/kube-apiserver
curl -# -L -o /usr/bin/kube-controller-manager $URL/kube-controller-manager
curl -# -L -o /usr/bin/kube-scheduler $URL/kube-scheduler
curl -# -L -o /usr/bin/kubectl $URL/kubectl
....

*Make them executable*

....
chmod +x /usr/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl}
....

*Binaries to be installed on Worker Node*

....
curl -# -L -o /usr/bin/kube-proxy $URL/kube-proxy
curl -# -L -o /usr/bin/kubelet $URL/kubelet
....

*Make them executable*

....
chmod +x /usr/bin/{kubelet,kube-proxy}
....

*Swap space*

The swap space on all kubernetes master and nodes should be disabled. If swap is
not disabled, kubelet service will not start on the master and nodes.
This is done to tightly pack instances. All deployments should be pinned with
CPU/memory limits. So if the scheduler sends a pod to a machine it should never
use swap space at all. Swap space slows down things. Its mainly done
for improving  performance.

....
swapoff -a
....

Also  comment the entries related to swap space in  /etc/fstab file so that
after a reboot also the swap space remains disabled. You can open the file and
comment the swap related lines or use the below command.

....
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
....

By default selinux and firewalld are disabled. You can check them using this

*SELINUX*

....
cat /etc/selinux/config
....

*FIREWALLD*
....
systemctl status firewalld
....

== Folders

*Make folders on master node*
....
mkdir -p /srv/kubernetes
mkdir -p /var/lib/flanneld
mkdir -p /var/lib/{kube-scheduler,kube-controller-manager}
....

*Make folders on worker node or nodes*
....
mkdir -p /srv/kubernetes
mkdir -p /var/lib/flanneld
mkdir -p /var/lib/{kubelet}
....

== Certificates
NOTE: Before you start doing this section find your ip address using the below command and replace it in the openssl configuration files below.

....
cd /srv/kubernetes
openssl genrsa -out ca-key.pem 2048
openssl req -x509 -new -nodes -key ca-key.pem -days 7200 -out ca.pem -subj "/CN=kube-ca"
....

*Openssl Config for generating certificates for master node*

....
cat > openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 127.0.0.1
IP.2 = <master_ip>
EOF
....

*Kube-apiserver certificates*

....
openssl genrsa -out apiserver-key.pem 2048
openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 7200 -extensions v3_req -extfile openssl.cnf
....

*Kube-controller certificates*

....
openssl genrsa -out kube-controller-manager-key.pem 2048
openssl req -new -key kube-controller-manager-key.pem -out kube-controller-manager.csr -subj "/CN=kube-controller-manager"
openssl x509 -req -in kube-controller-manager.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-controller-manager.pem -days 7200
....

*Kube-scheduler certificates*

....
openssl genrsa -out kube-scheduler-key.pem 2048
openssl req -new -key kube-scheduler-key.pem -out kube-scheduler.csr -subj "/CN=kube-scheduler"
openssl x509 -req -in kube-scheduler.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-scheduler.pem -days 7200
....

*Admin certificates*

....
openssl genrsa -out admin-key.pem 2048
openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=admin"
openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 7200
....

*Kube-proxy certificates*

....
openssl genrsa -out kube-proxy-key.pem 2048
openssl req -new -key kube-proxy-key.pem -out kube-proxy.csr -subj "/CN=kube-proxy"
openssl x509 -req -in kube-proxy.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-proxy.pem -days 7200
....

*Kubelet certificates*

....
openssl genrsa -out kubelet-key.pem 2048
openssl req -new -key kubelet-key.pem -out kubelet.csr -subj "/CN=kubelet"
openssl x509 -req -in kubelet.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kubelet.pem -days 7200
....


*Openssl Config for generating certificates for worker node*
....
cat > worker_openssl.cnf << EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = $WORKER_IP
EOF
....

*Worker Certificates*

....
openssl genrsa -out rhel-worker-key.pem 2048
openssl req -new -key rhel-worker-key.pem -out rhel-worker.csr -subj "/CN=rhel" -config worker_openssl.cnf
openssl x509 -req -in rhel-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out rhel-worker.pem -days 7200 -extensions v3_req -extfile worker_openssl.cnf
....

*Config file for generating etcd peer certificates*

....
cat > etcd_openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = clientAuth,serverAuth
subjectAltName = @alt_names
[alt_names]
IP.1 = $MASTER_IP
EOF
....

*Etcd certificates*

....
openssl genrsa -out etcd.key 2048
openssl req -new -key etcd.key -out etcd.csr -subj "/CN=etcd" -extensions v3_req -config etcd-openssl.cnf -sha256
openssl x509 -req -sha256 -CA ca.pem -CAkey ca-key.pem -CAcreateserial -in etcd.csr -out etcd.crt -extensions v3_req -extfile openssl-etcd.cnf -days 7200
....

== Configuation files
*Kubeconfig files for various components*

*admin kubeconfig*

....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.122.219:6443
kubectl config set-credentials admin --client-certificate=/srv/kubernetes/admin.pem --client-key=/srv/kubernetes/admin-key.pem --embed-certs=true --token=$TOKEN
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=admin
kubectl config use-context linux1.k8s
cat ~/.kube/config
....

*kube-controller kubeconfig*

....
mkdir -p /var/lib/kube-controller-manager
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.122.219:6443 --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-credentials kube-controller-manager --client-certificate=/srv/kubernetes/kube-controller-manager.pem --client-key=/srv/kubernetes/kube-controller-manager-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-controller-manager --kubeconfig=/var/lib/kube-controller-manager/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
....

*kube-scheduler kubeconfig*

....
mkdir -p /var/lib/kube-scheduler
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.122.219:6443 --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-credentials kube-scheduler --client-certificate=/srv/kubernetes/kube-scheduler.pem --client-key=/srv/kubernetes/kube-scheduler-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-scheduler --kubeconfig=/var/lib/kube-scheduler/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-scheduler/kubeconfig
....

*kubelet kubeconfig*

....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.122.219:6443 --kubeconfig=kubelet.kubeconfig
kubectl config set-credentials kubelet --client-certificate=/srv/kubernetes/kubelet.pem --client-key=/srv/kubernetes/kubelet-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=kubelet.kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kubelet --kubeconfig=/var/lib/kubelet/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=kubelet.kubeconfig
scp kube-proxy.kubeconfig root@192.168.122.16:/var/lib/kubelet/kubeconfig
....

*kube-proxy kubeconfig*

....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.122.219:6443 --kubeconfig=kube-proxy.kubeconfig
kubectl config set-credentials kube-proxy --client-certificate=/srv/kubernetes/kube-proxy.pem --client-key=/srv/kubernetes/kube-proxy-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=kube-proxy.kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-proxy --kubeconfig=/var/lib/kube-proxy/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=kube-proxy.kubeconfig
scp kube-proxy.kubeconfig root@192.168.122.16:/var/lib/kube-proxy/kubeconfig
....


== Etcd
*About etcd*
....
It is a distributed storage device used to store the state of the cluster. All other components are stateless. A state is stored in the
form of key-value pair.
....

Installation

....
yum install etcd
....
Also edit the service file in the /usr/lib/systemd/system/etcd.service

....
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
Environment="ETCD_UNSUPPORTED_ARCH=S390X"
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors

ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\"${ETCD_NAME}\"  \
--data-dir=\"${ETCD_DATA_DIR}\" \
--listen-client-urls=\"${ETCD_LISTEN_CLIENT_URLS}\" \
--cert-file=\"${ETCD_CERT_FILE}\" \
--key-file=\"${ETCD_KEY_FILE}\" \
--peer-cert-file=\"${ETCD_PEER_CERT_FILE}\" \
--peer-key-file=\"${ETCD_PEER_KEY_FILE}\" \
--trusted-ca-file=\"${ETCD_TRUSTED_CA_FILE}\"  \
--peer-trusted-ca-file=\"${ETCD_TRUSTED_CA_FILE}\"  \
--peer-client-cert-auth \
--client-cert-auth \
--initial-advertise-peer-urls=\"${ETCD_INITIAL_ADVERTISE_PEER_URLS}\"  \
--listen-peer-urls=\"${ETCD_LISTEN_PEER_URLS}\" \
--advertise-client-urls=\"${ETCD_ADVERTISE_CLIENT_URLS}\"  \
--initial-cluster-token=\"${ETCD_INITIAL_CLUSTER_TOKEN}\" \
--initial-cluster=\"${ETCD_INITIAL_CLUSTER}\" \
--initial-cluster-state=\"${ETCD_INITIAL_CLUSTER_STATE}\""

Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
....

The configuration file in the /etc/etcd/etcd.conf should look like this.

....
#[Member]
#ETCD_CORS=""
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
#ETCD_WAL_DIR=""
ETCD_LISTEN_PEER_URLS="http://148.100.98.173:2380"
ETCD_LISTEN_CLIENT_URLS="http://148.100.98.173:2379"
#ETCD_MAX_SNAPSHOTS="5"
#ETCD_MAX_WALS="5"
ETCD_NAME="default"
#ETCD_SNAPSHOT_COUNT="100000"
#ETCD_HEARTBEAT_INTERVAL="100"
#ETCD_ELECTION_TIMEOUT="1000"
#ETCD_QUOTA_BACKEND_BYTES="0"
#ETCD_MAX_REQUEST_BYTES="1572864"
#ETCD_GRPC_KEEPALIVE_MIN_TIME="5s"
#ETCD_GRPC_KEEPALIVE_INTERVAL="2h0m0s"
#ETCD_GRPC_KEEPALIVE_TIMEOUT="20s"
#
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://148.100.98.173:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://148.100.98.173:2379"
#ETCD_DISCOVERY=""
#ETCD_DISCOVERY_FALLBACK="proxy"
#ETCD_DISCOVERY_PROXY=""
#ETCD_DISCOVERY_SRV=""
ETCD_INITIAL_CLUSTER="default=http://148.100.98.173:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
#ETCD_STRICT_RECONFIG_CHECK="true"
#ETCD_ENABLE_V2="true"
#
#[Proxy]
#ETCD_PROXY="off"
#ETCD_PROXY_FAILURE_WAIT="5000"
#ETCD_PROXY_REFRESH_INTERVAL="30000"
#ETCD_PROXY_DIAL_TIMEOUT="1000"
#ETCD_PROXY_WRITE_TIMEOUT="5000"
#ETCD_PROXY_READ_TIMEOUT="0"
#
#[Security]
#ETCD_CERT_FILE="/srv/kubernetes/etcd.crt"
#ETCD_KEY_FILE="/srv/kubernetes/etcd.key"
#ETCD_CLIENT_CERT_AUTH="true"
#ETCD_TRUSTED_CA_FILE="/srv/kubernetes/ca.pem"
#ETCD_AUTO_TLS="false"
#ETCD_PEER_CERT_FILE="/srv/kubernetes/etcd.crt"
#ETCD_PEER_KEY_FILE="/srv/kubernetes/etcd.key"
#ETCD_PEER_CLIENT_CERT_AUTH="true"
#ETCD_PEER_TRUSTED_CA_FILE=""
#ETCD_PEER_AUTO_TLS="false"
#
#[Logging]
ETCD_DEBUG="true"
ETCD_LOG_PACKAGE_LEVELS="DEBUG"
#ETCD_LOG_OUTPUT="default"
#
#[Unsafe]
#ETCD_FORCE_NEW_CLUSTER="false"
#
#[Version]
#ETCD_VERSION="false"
#ETCD_AUTO_COMPACTION_RETENTION="0"
#
#[Profiling]
#ETCD_ENABLE_PPROF="false"
#ETCD_METRICS="basic"
#
#[Auth]
#ETCD_AUTH_TOKEN="simple"
....


NOTE: Before you start your etcd instance make sure you flush your iptables.

....
iptables --flush
....

*Commands to start, check status of etcd*

....
systemctl enable etcd
systemctl start etcd
systemctl status etcd
....

*Testing etcd*
....
etcdctl --endpoints https://148.100.98.173:2379 \
  --ca-file=/srv/kubernetes/ca.pem \
  --cert-file=/srv/kubernetes/etcd.crt \
  --key-file=/srv/kubernetes/etcd.key \
  cluster-health
....

Output
....
member ddd902eabd9d33b7 is healthy: got healthy result from http://148.100.98.173:2379
cluster is healthy
....

....
etcdctl --endpoints https://148.100.98.173:2379 \
  --ca-file=/srv/kubernetes/ca.pem \
  --cert-file=/srv/kubernetes/etcd.crt \
  --key-file=/srv/kubernetes/etcd.key \
  member list
....

Output
....
ddd902eabd9d33b7: name=default peerURLs=http://148.100.98.173:2380 clientURLs=http://148.100.98.173:2379 isLeader=true
....


NOTE: At times when you try to restart etcd service it may not get started because the previous instance might not have been killed. So using netstat to check that and find the PID and kill it.

== Docker
You need not install *Docker CE* because it is present in /usr/local/bin folder. But this docker does have a service file related to it.
So remove the existing service file and create a new one.

....
rm -rf /usr/lib/systemd/system/docker*  && /etc/systemd/system/docker.service.d/
....

Make an empty file in */etc/sysconfig/* with name *docker* so later you can add the docker configuration options.

Now create both of the following files in /etc/systemd/system/

*docker.service*
....
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.com
After=network.target docker.socket
Requires=docker.socket

[Service]
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
EnvironmentFile=/etc/sysconfig/docker
PIDFile=/var/run/docker.pid
ExecStart=/usr/local/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 -G docker
#ExecStart=/usr/local/bin/dockerd -H --storage-driver=devicemapper fd:// $DOCKER_OPTS
MountFlags=slave
LimitNOFILE=1048576
LimitNPROC=1048576
LimitCORE=infinity
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

[Install]
WantedBy=multi-user.target
....

*docker.socket*
....
[Unit]
Description=Docker Socket for the API
PartOf=docker.service

[Socket]
ListenStream=/var/run/docker.sock
SocketMode=0660
# A Socket(User|Group) replacement workaround for systemd <= 214
ExecStartPost=/usr/bin/chown root:docker /var/run/docker.sock
....

Reload configuration, remove the previous symbolic links, form new ones

....
systemctl daemon-reload && systemctl disable docker.service && systemctl enable docker.service
....

....
groupadd docker
....

....
usermod -aG docker linux1 && usermod -aG docker root
....

*Command for getting docker started* +

....
systemctl start docker
....

== Flannel

Flannel is networking overlay layer designed for kubernetes but it is also used as a general purpose SDN.
Flannel does this by creating a flat network over the entire cluster which runs above the host network overlay network. So it has to be installed on all the nodes.
So by this overlay network each container gets an IP and this makes the container to container communication easy. If two containers are on the same machine then
they can use the docker bridge otherwise they use the encaptulation and UDP in order to communicate with each other.

....
yum install flannel
....

Flannel does use etcd for mapping subnet to host. So by running the below command etcd will know that flannel will use it's service.
....
etcdctl --endpoints https://148.100.98.173 --cert-file /srv/kubernetes/etcd.crt --key-file /srv/kubernetes/etcd.key --ca-file /srv/kubernetes/ca.pem set /coreos.com/network/config '{ "Network": "100.64.0.0/16", "SubnetLen": 24, "Backend": {"Type": "vxlan"} }'
....



*Commands to start, check status flanneld*

....
systemctl enable flanneld
systemctl start flanneld
systemctl status flanneld
....

*Kube-apiserver*

*About kube-apiserver*

Its role is to put the stuff into the etcd.

*Setting up kube-apiserver as systemd service*

....
cat > /etc/systemd/system/kube-apiserver.service << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target etcd.service flanneld.service

[Service]
EnvironmentFile=-/var/lib/flanneld/subnet.env
ExecStart=/usr/bin/kube-apiserver \\
 --bind-address=0.0.0.0 \\
 --advertise-address=148.100.99.252 \\
 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota \\
 --anonymous-auth=false \\
 --apiserver-count=1 \\
 --authorization-mode=Node,RBAC,AlwaysAllow \\
 --authorization-rbac-super-user=admin \\
 --etcd-cafile=/srv/kubernetes/ca.pem \\
 --etcd-certfile=/srv/kubernetes/etcd.crt \\
 --etcd-keyfile=/srv/kubernetes/etcd.key \\
 --etcd-servers=https://148.100.99.252:2379 \\
 --enable-swagger-ui=true \\
 --insecure-bind-address=0.0.0.0 \\
 --kubelet-certificate-authority=/srv/kubernetes/ca.pem \\
 --kubelet-client-certificate=/srv/kubernetes/kubelet.pem \\
 --kubelet-client-key=/srv/kubernetes/kubelet-key.pem \\
 --kubelet-https=true \\
 --client-ca-file=/srv/kubernetes/ca.pem \\
 --runtime-config=api/all=true,batch/v2alpha1=true,rbac.authorization.k8s.io/v1alpha1=true \\
 --secure-port=6443 \\
 --service-cluster-ip-range=100.65.0.0/24 \\
 --storage-backend=etcd3 \\
 --tls-cert-file=/srv/kubernetes/apiserver.pem \\
 --tls-private-key-file=/srv/kubernetes/apiserver-key.pem \\
 --tls-ca-file=/srv/kubernetes/ca.pem
 --logtostderr=true
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kube-apiserver*
....
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
....

*kubectl version*
This should output the version of both the client and the server after the server starts working.

....
kubectl version
....

Output

....
Client Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.8", GitCommit:"c138b85178156011dc934c2c9f4837476876fb07", GitTreeState:"clean", BuildDate:"2018-05-21T19:01:12Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/s390x"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.8", GitCommit:"c138b85178156011dc934c2c9f4837476876fb07", GitTreeState:"clean", BuildDate:"2018-05-21T18:53:18Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/s390x"}
....

For monitoring logs of kube-apiserver (optional)

....
journalctl -fu kube-apiserver
....


NOTE:  Monitor the logs when REST calls get exchange which indicates that the server has started.

*Setting up Kube-scheduler*

....
cat > /etc/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/bin/kube-scheduler \\
  --leader-elect=true \\
  --kubeconfig=/var/lib/kube-scheduler/kubeconfig \\
  --master=https://148.100.98.173:6443
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

....

*Commands to start and check status of kube-scheduler*

....
sudo systemctl enable kube-scheduler
sudo systemctl start kube-scheduler
sudo systemctl status kube-scheduler
....


*Setting up Kube-controller manager*

....
cat > /etc/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --allocate-node-cidrs=true \\
	--attach-detach-reconcile-sync-period=1m0s \\
	--cluster-cidr=100.64.0.0/16 \\
	--cluster-name=k8s.virtual.local \\
  --cluster-signing-cert-file=/srv/kubernetes/ca.pem \\
  --cluster-signing-key-file=/srv/kubernetes/ca-key.pem \\
  --configure-cloud-routes=false \\
  --kubeconfig=/var/lib/kube-controller-manager/kubeconfig \\
	--leader-elect=true \\
  --master=https://148.100.98.173:6443 \\
	--root-ca-file=/srv/kubernetes/ca.pem \\
	--service-account-private-key-file=/srv/kubernetes/apiserver-key.pem \\
  --service-cluster-ip-range=100.65.0.0/24 \\
  --use-service-account-credentials=true
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kube-controller-manager*

....
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
....

Now we can do a health check of the master node
....
Kubectl get cs
....

Output
....
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}
....

*kubelet*
Its role is to run the pods on the worker node.

*Setting up Kubelet as a systemd service*

....
cat > /etc/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/bin/kubelet \\
  --allow-privileged=true \\
  --cluster-dns=100.65.0.10 \\
  --cluster-domain=cluster.local \\
  --container-runtime=docker \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --serialize-image-pulls=false \\
  --register-node=true \\
  --tls-cert-file=/srv/kubernetes/kubelet.pem \\
  --tls-private-key-file=/srv/kubernetes/kubelet-key.pem
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kubelet*

....
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
....

*Setting up Kube-proxy as a systemd service*

....
cat > /etc/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/bin/kube-proxy \\
  --cluster-cidr=100.64.0.0/16 \\
  --masquerade-all=true \\
  --kubeconfig=/var/lib/kube-proxy/kubeconfig \\
  --proxy-mode=iptables
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kube-proxy*

....
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy
....

Now we need to check whether the node has been registered or not.

....
kubectl get nodes
....

Output
....
NAME                   STATUS    ROLES     AGE       VERSION
kubeworker.novalocal   Ready     <none>    1d        v1.9.8
....

NOTE: The flags of the respective service files may get deprecated between various releases of kubernetes. If a particular component
throws an error of deprecated flag on checking the logs then please refer to the official API documentation of that particular version and modify the service file
accordingly.


Now the kubernetes cluster is ready

You can deploy the app of your choice.

== References

1. https://github.com/linux-on-ibm-z/docs/wiki/Building-etcd
2. https://nixaid.com/deploying-kubernetes-cluster-from-scratch/
3. https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step/
4. https://kubernetes.io/docs/setup/scratch/
5. https://docs.docker.com/install/linux/linux-postinstall/
6. https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/deployments/kube-dns.yaml
7. https://docs.platform9.com/support/disabling-swap-kubernetes-node/
8. https://serverfault.com/questions/881517/why-disable-swap-on-kubernetes
9. https://github.com/anujajakhade/anuja/wiki/Docker-on-RHEL
